{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pyyaml h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7QMCxLELGQ2T"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from collections import Counter\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "WIDTH = 320\n",
        "HEIGHT = 180"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxo8J7sBXGHg"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3OdzN5oLLYiO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3375\n"
          ]
        }
      ],
      "source": [
        "PATH = r\"/home/fizzer/cnn_trainer/data/\"\n",
        "labels_raw = os.listdir(PATH)\n",
        "labels = labels_raw\n",
        "print(len(labels))\n",
        "\n",
        "\n",
        "# GOOGLE COLAB\n",
        "# # You will need to update this path to match the folder in your Google Drive\n",
        "# PATH = \"/content/data/\"\n",
        "# labels_raw = !ls \"{PATH}\"\n",
        "# labels_tot = []\n",
        "# for label in labels_raw:\n",
        "#     for im in label.split():\n",
        "#         labels_tot.append(im)\n",
        "# print(np.size(labels_tot))\n",
        "# print(labels_raw[-1])\n",
        "# print(labels_tot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KFr95F2rXVQy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3375, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fizzer/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2009: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  result = asarray(a).shape\n"
          ]
        }
      ],
      "source": [
        "# Load the images and assign label 0 and 1\n",
        "# imgset0 = np.array([[np.array(Image.open(f'{PATH}/{file}')), 0]\n",
        "#                     for file in labels[:]])\n",
        "\n",
        "imgset0 = []\n",
        "for label in labels:\n",
        "    # imgray = cv2.cvtColor(np.array(Image.open(f'{PATH}/{label}')), cv2.COLOR_BGR2GRAY)\n",
        "    thresh = 200\n",
        "    im_bw = cv2.threshold(np.array(Image.open(f'{PATH}/{label}')), thresh, 255, cv2.THRESH_BINARY)[1]\n",
        "    dim = (320, 180)\n",
        "    im_rs = cv2.resize(im_bw, dim, interpolation = cv2.INTER_AREA)\n",
        "    if label[-5] == 'R':\n",
        "        action = -1\n",
        "    elif label[-5] == 'F':\n",
        "        action = 0\n",
        "    else:\n",
        "        action = 1\n",
        "    imgset0.append([im_rs, action])\n",
        "\n",
        "print(np.shape(imgset0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PUvVoi-TXfFR"
      },
      "outputs": [],
      "source": [
        "all_dataset = imgset0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OEvhp0rRBlzU"
      },
      "outputs": [],
      "source": [
        "# Shuffle the dataset\n",
        "np.random.shuffle(all_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QeZRVTeXnsv",
        "outputId": "f05593a2-ead0-474b-f039-e01d57809688"
      },
      "outputs": [],
      "source": [
        "# Generate X and Y datasets\n",
        "X_dataset_orig = np.array([data[0] for data in all_dataset[:]], dtype=object)\n",
        "Y_dataset_orig = np.array([[data[1]] for data in all_dataset]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3Y8kWMW8XrJ5"
      },
      "outputs": [],
      "source": [
        "NUMBER_OF_LABELS = 3\n",
        "CONFIDENCE_THRESHOLD = 0.01\n",
        "\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)].T\n",
        "    return Y\n",
        "# Normalize X (images) dataset\n",
        "X_dataset = X_dataset_orig\n",
        "for i in range(len(X_dataset_orig)):\n",
        "    X_dataset[i] = X_dataset[i]/255.\n",
        "\n",
        "# Convert Y dataset to one-hot encoding\n",
        "Y_dataset = convert_to_one_hot(Y_dataset_orig, NUMBER_OF_LABELS).T\n",
        "X_dataset_init = X_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cCD1GITRXvL8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total examples: 3375\n",
            "Training examples: 2700\n",
            "Test examples: 675\n",
            "X shape: (3375, 180, 320)\n",
            "Y shape: (3375, 2)\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "print(\"Total examples: {:d}\\nTraining examples: {:d}\\nTest examples: {:d}\".\n",
        "      format(X_dataset.shape[0],\n",
        "             math.ceil(X_dataset.shape[0] * (1-VALIDATION_SPLIT)),\n",
        "             math.floor(X_dataset.shape[0] * VALIDATION_SPLIT)))\n",
        "print(\"X shape: \" + str(X_dataset.shape))\n",
        "print(\"Y shape: \" + str(Y_dataset.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_dataset_rs = np.asarray(X_dataset).astype(np.float32).reshape((X_dataset.shape[0], 180, 320, 1))\n",
        "Y_dataset_rs = np.asarray(Y_dataset).astype(np.float32).reshape(Y_dataset.shape[0], 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zmb2tR9X2FP"
      },
      "source": [
        "## Train CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2lcCeKWSX68r"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnJDVDCWUomK"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9HmypEgxX9jt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-20 23:13:37.549541: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/fizzer/.local/lib/python3.8/site-packages/cv2/../../lib64:/opt/ros/noetic/lib\n",
            "2022-11-20 23:13:37.549564: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-11-20 23:13:37.549584: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (skynet): /proc/driver/nvidia/version does not exist\n",
            "2022-11-20 23:13:37.554071: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-11-20 23:13:37.772239: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2599990000 Hz\n",
            "2022-11-20 23:13:37.772915: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd254000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-11-20 23:13:37.772942: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-11-20 23:13:38.622067: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 44826624 exceeds 10% of free system memory.\n",
            "2022-11-20 23:13:38.672369: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 44826624 exceeds 10% of free system memory.\n",
            "2022-11-20 23:13:38.692462: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 44826624 exceeds 10% of free system memory.\n"
          ]
        }
      ],
      "source": [
        "conv_model = models.Sequential()\n",
        "conv_model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                             input_shape=(HEIGHT, WIDTH, 1)))\n",
        "conv_model.add(layers.MaxPooling2D((2, 2)))\n",
        "conv_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "conv_model.add(layers.MaxPooling2D((2, 2)))\n",
        "conv_model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "conv_model.add(layers.MaxPooling2D((2, 2)))\n",
        "conv_model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "conv_model.add(layers.MaxPooling2D((2, 2)))\n",
        "conv_model.add(layers.Flatten())\n",
        "conv_model.add(layers.Dropout(0.5))\n",
        "conv_model.add(layers.Dense(512, activation='relu'))\n",
        "conv_model.add(layers.Dense(3, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8E0vmjNyYAro"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 178, 338, 32)      320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 89, 169, 32)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 87, 167, 64)       18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 43, 83, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 41, 81, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 20, 40, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 18, 38, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 9, 19, 128)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 21888)             0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 21888)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               11207168  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 11,448,450\n",
            "Trainable params: 11,448,450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "conv_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oktNuZifYep9"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 1e-4\n",
        "conv_model.compile(loss='binary_crossentropy',\n",
        "                   optimizer=optimizers.RMSprop(learning_rate=LEARNING_RATE),\n",
        "                   metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wV33OYHmYgjC"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history_conv \u001b[39m=\u001b[39m conv_model\u001b[39m.\u001b[39;49mfit(X_dataset[:\u001b[39m10\u001b[39;49m], Y_dataset[:\u001b[39m10\u001b[39;49m], \n\u001b[1;32m      2\u001b[0m                               validation_split\u001b[39m=\u001b[39;49mVALIDATION_SPLIT, \n\u001b[1;32m      3\u001b[0m                               epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[1;32m      4\u001b[0m                               batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:66\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_method_wrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     65\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_multi_worker_mode():  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     68\u001b[0m   \u001b[39m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m   \u001b[39mif\u001b[39;00m dc_context\u001b[39m.\u001b[39mget_current_worker_context():\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_call_args(\u001b[39m'\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    791\u001b[0m \u001b[39mif\u001b[39;00m validation_split:\n\u001b[1;32m    792\u001b[0m   \u001b[39m# Create the validation data using the training data. Only supported for\u001b[39;00m\n\u001b[1;32m    793\u001b[0m   \u001b[39m# `Tensor` and `NumPy` input.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m   (x, y, sample_weight), validation_data \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 795\u001b[0m       data_adapter\u001b[39m.\u001b[39;49mtrain_validation_split((x, y, sample_weight),\n\u001b[1;32m    796\u001b[0m                                           validation_split\u001b[39m=\u001b[39;49mvalidation_split,\n\u001b[1;32m    797\u001b[0m                                           shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n\u001b[1;32m    799\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope(), \\\n\u001b[1;32m    800\u001b[0m      training_utils\u001b[39m.\u001b[39mRespectCompiledTrainableState(\u001b[39mself\u001b[39m):\n\u001b[1;32m    801\u001b[0m   \u001b[39m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m   data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mDataHandler(\n\u001b[1;32m    803\u001b[0m       x\u001b[39m=\u001b[39mx,\n\u001b[1;32m    804\u001b[0m       y\u001b[39m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m       use_multiprocessing\u001b[39m=\u001b[39muse_multiprocessing,\n\u001b[1;32m    815\u001b[0m       model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py:1337\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   1334\u001b[0m   t \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor_v2(t)\n\u001b[1;32m   1335\u001b[0m   \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39mgather_v2(t, indices)\n\u001b[0;32m-> 1337\u001b[0m train_arrays \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39;49mmap_structure(\n\u001b[1;32m   1338\u001b[0m     functools\u001b[39m.\u001b[39;49mpartial(_split, indices\u001b[39m=\u001b[39;49mtrain_indices), arrays)\n\u001b[1;32m   1339\u001b[0m val_arrays \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m   1340\u001b[0m     functools\u001b[39m.\u001b[39mpartial(_split, indices\u001b[39m=\u001b[39mval_indices), arrays)\n\u001b[1;32m   1342\u001b[0m \u001b[39mreturn\u001b[39;00m train_arrays, val_arrays\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py:617\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m flat_structure \u001b[39m=\u001b[39m [flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure]\n\u001b[1;32m    614\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    616\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    618\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py:617\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m flat_structure \u001b[39m=\u001b[39m [flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure]\n\u001b[1;32m    614\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    616\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    618\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py:1334\u001b[0m, in \u001b[0;36mtrain_validation_split.<locals>._split\u001b[0;34m(t, indices)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[39mif\u001b[39;00m t \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1333\u001b[0m   \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m-> 1334\u001b[0m t \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor_v2(t)\n\u001b[1;32m   1335\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39mgather_v2(t, indices)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1278\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m   1221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_tensor_v2\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype_hint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1222\u001b[0m   \u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m \n\u001b[1;32m   1224\u001b[0m \u001b[39m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[39m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1278\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_to_tensor(\n\u001b[1;32m   1279\u001b[0m       value\u001b[39m=\u001b[39;49mvalue,\n\u001b[1;32m   1280\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1281\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1282\u001b[0m       preferred_dtype\u001b[39m=\u001b[39;49mdtype_hint,\n\u001b[1;32m   1283\u001b[0m       as_ref\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1341\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor did not convert to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1337\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mthe preferred dtype: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1338\u001b[0m                       (ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, preferred_dtype\u001b[39m.\u001b[39mbase_dtype))\n\u001b[1;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1341\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1343\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1344\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:52\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     51\u001b[0m   \u001b[39mdel\u001b[39;00m as_ref  \u001b[39m# Unused.\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m   \u001b[39mreturn\u001b[39;00m constant_op\u001b[39m.\u001b[39;49mconstant(value, dtype, name\u001b[39m=\u001b[39;49mname)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:261\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \n\u001b[1;32m    168\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    262\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:270\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m ctx \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m ctx\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 270\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    271\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:96\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m     95\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 96\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
          ]
        }
      ],
      "source": [
        "history_conv = conv_model.fit(X_dataset, Y_dataset, \n",
        "                              validation_split=VALIDATION_SPLIT, \n",
        "                              epochs=1, \n",
        "                              batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6XTABclYmZe"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history_conv.history['loss'])\n",
        "plt.plot(history_conv.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train loss', 'val loss'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQttXMJFYqir"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history_conv.history['acc'])\n",
        "plt.plot(history_conv.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy (%)')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train accuracy', 'val accuracy'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EDwRy0oYyXn"
      },
      "source": [
        "## Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87Katd2fYzA8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b3c6bc12c3b46d99a5fc48ede6745bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(IntSlider(value=10, description='index', max=10), Output()), _dom_classes=('widget-inter"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<function __main__.displayImage(index)>"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ipywidgets import interact\n",
        "import ipywidgets as ipywidgets\n",
        "\n",
        "# Display images in the training data set. \n",
        "def displayImage(index):\n",
        "  img = X_dataset[index]\n",
        "  \n",
        "  img_aug = np.expand_dims(img, axis=0)\n",
        "  y_predict = conv_model.predict(img_aug)[0]\n",
        "  \n",
        "  plt.imshow(img)  \n",
        "  caption = (\"                Dot | No Dots\\n\"+\n",
        "             \"GND truth: {:.2} | {:.2}\\nPredicted: {:.2} | {:.2}\".\n",
        "             format(Y_dataset[index][0], Y_dataset[index][1], y_predict[0], y_predict[1]))\n",
        "  plt.text(0.5, 0.5, caption, \n",
        "           color='orange', fontsize = 16,\n",
        "           horizontalalignment='left', verticalalignment='bottom')\n",
        "\n",
        "\n",
        "interact(displayImage, \n",
        "        index=ipywidgets.IntSlider(min=0, max=X_dataset_orig.shape[0],\n",
        "                                   step=1, value=10))\n",
        "#displayImage(3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "8eff614b2f78753597a1aee0d283ed53e130ad8e0c04eb926549f07842b39abe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
